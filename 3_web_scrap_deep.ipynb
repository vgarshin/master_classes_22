{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-friday",
   "metadata": {},
   "source": [
    "# Mastering Applied Skills in Management, Analytics and Entrepreneurship I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-speech",
   "metadata": {},
   "source": [
    "## DATA COLLECTION TECHNIQUES\n",
    "## Part IV. Web scraping deeper dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-lingerie",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "from random import randint, uniform\n",
    "# for sending requests\n",
    "from urllib.request import (\n",
    "    Request, \n",
    "    urlopen, \n",
    "    URLError, \n",
    "    HTTPError, \n",
    "    ProxyHandler, \n",
    "    build_opener, \n",
    "    install_opener)\n",
    "# to parce html data\n",
    "from bs4 import BeautifulSoup\n",
    "# for time delay while scraping\n",
    "from time import sleep, gmtime, strftime\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import quote, unquote\n",
    "# to work with the data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-single",
   "metadata": {},
   "source": [
    "### 2. Tools and hints for requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 YaBrowser/19.6.1.153 Yowser/2.5 Safari/537.36'\n",
    "MIN_TIME_SLEEP = .1\n",
    "MAX_TIME_SLEEP = 2\n",
    "MAX_COUNTS = 2\n",
    "TIMEOUT = 5\n",
    "MAX_PAGES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_lite(url_page, timeout):\n",
    "    # sleep a while for not to overload site\n",
    "    sleep(uniform(MIN_TIME_SLEEP, MAX_TIME_SLEEP))\n",
    "    # make a request\n",
    "    request = Request(url_page)\n",
    "    request.add_header('User-Agent', USER_AGENT)\n",
    "    # get the response\n",
    "    response = urlopen(request, timeout=timeout)\n",
    "    content = response.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_main = 'https://piter-online.net/address'\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-cattle",
   "metadata": {},
   "source": [
    "### 3. How to work with soup, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', class_=\"app131\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in soup.find_all('div'):\n",
    "    print('*' * 50)\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', class_=\"app131\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in soup.find_all('div', class_=\"app131\"):\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a', attrs={'datatest': 'top_provider_block'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a', attrs={'datatest': re.compile(r'top')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-refrigerator",
   "metadata": {},
   "source": [
    "### 3. Let's cook our soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-tumor",
   "metadata": {},
   "source": [
    "We can search with `CTRL+F` because of the power of Jupyter notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-management",
   "metadata": {},
   "source": [
    "#### Step 1. Regions of the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# <div class=\"app281\"><a href=\"/address/адмиралтейский-id1192\">Адмиралтейский</a></div>\n",
    "# \n",
    "soup.find('div', attrs={'class': 'app281'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or another way\n",
    "soup.find('div', class_ = 'app281')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find a url\n",
    "soup.find('div', class_ = 'app281').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', class_ = 'app281').a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://piter-online.net' + soup.find('div', class_ = 'app281').a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now will find all regions of the city\n",
    "soup.find_all('div', class_ = 'app281')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_ = 'app281'):\n",
    "    print(item.text, 'https://piter-online.net' + item.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-youth",
   "metadata": {},
   "source": [
    "#### Step 2. Streets of the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's looks at one region\n",
    "url_main = 'https://piter-online.net' + quote(soup.find_all('div', class_ = 'app281')[0].a['href'])\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# <div class=\"app321 app322\">\n",
    "# <a href=\"/address/адмиралтейский-id1192/ул-1-я-конная-лахта-id377844\"> 1-я Конная Лахта ул</a>\n",
    "# </div>\n",
    "# \n",
    "soup.find('div', attrs={'class': 'app321'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_ = 'app321'):\n",
    "    print(item.text, 'https://piter-online.net' + item.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-tuner",
   "metadata": {},
   "source": [
    "#### Step 3. Houses on the street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's looks at one street\n",
    "url_main = 'https://piter-online.net' + quote(soup.find_all('div', class_ = 'app321')[0].a['href'])\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_ = 'app258'):\n",
    "    url_house = 'https://piter-online.net' + item.a['href']\n",
    "    house_id = url_house[url_house.find('=') + 1 : ]\n",
    "    print(item.text, house_id, url_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-forwarding",
   "metadata": {},
   "source": [
    "#### Step 3. Houses on the street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's looks at one street\n",
    "page = 1\n",
    "url_main = f'https://piter-online.net/rates/{page}?house_id={house_id}'\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# <div class=\"app297\"><a href=\"/providers/rostelecom/rates\">Ростелеком</a></div>\n",
    "#\n",
    "soup.find('div', attrs={'class': 'app297'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', attrs={'class': 'app372'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_ = 'app297'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item0, item1 in zip(\n",
    "    soup.find_all('div', class_ = 'app297'),\n",
    "    soup.find_all('div', class_ = 'app372')\n",
    "):\n",
    "    # 1st try\n",
    "    print(item0.text, item1.text)\n",
    "    # 2nd try\n",
    "    #print(item0.text, item1.span)\n",
    "    # 3rd\n",
    "    #print(item0.text, item1.span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-sight",
   "metadata": {},
   "source": [
    "## <font color='red'>INTERMEDIATE QUIZ</font>\n",
    "We lost discount price from the data, so:\n",
    "1. Look at the table on the web site and find discount price in our soup\n",
    "2. Update code to print the discount price along with the base price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-assistant",
   "metadata": {},
   "source": [
    "#### Step 4. Combine all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh our soup for full load\n",
    "url_main = 'https://piter-online.net/address'\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters for debug\n",
    "DEBUG = True\n",
    "end = 2 if DEBUG else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place to store the data\n",
    "addresses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- first loop is for data on the regions ---\n",
    "for reg_data in tqdm(soup.find_all('div', class_='app281')[:end], \n",
    "                     desc='regions'):\n",
    "    url_reg = url_main.replace('/address', '') + quote(reg_data.find('a')['href'])\n",
    "    html = get_content_lite(url_reg, timeout=TIMEOUT)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # second loop is for streets ---\n",
    "    for street_data in tqdm(soup.find_all('div', class_='app321')[:end], \n",
    "                            desc='streets of '+ reg_data.getText()):\n",
    "        url_street = url_main.replace('/address', '') + quote(street_data.find('a')['href'])\n",
    "        html = get_content_lite(url_street, timeout=TIMEOUT)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # --- this loop is for houses ---\n",
    "        for houses_data in tqdm(soup.find_all('div', class_='app258')[:end], \n",
    "                                desc='houses of ' + street_data.getText()):\n",
    "            url_house = url_main.replace('/address', '') + quote(houses_data.find('a')['href'])\n",
    "            url_house_ = unquote(url_house)\n",
    "            house_id = url_house_[url_house_.find('=') + 1 : ]\n",
    "            # --- NOTE here we use loop for all pages for one house ---\n",
    "            for page in range(1, MAX_PAGES):\n",
    "                url_provs = f'https://piter-online.net/rates/{page}?house_id={house_id}'\n",
    "                html = get_content_lite(url_provs, timeout=TIMEOUT)\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                if soup.find_all('div', class_='app271') == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    for prov_name, price in zip(\n",
    "                        soup.find_all('div', class_='app297')[:end],\n",
    "                        soup.find_all('div', class_ = 'app372')[:end]\n",
    "                    ):\n",
    "                        # --- FINALLY collect all the data in a form of dictionary ---\n",
    "                        addresses.append(\n",
    "                            {\n",
    "                                'region': reg_data.getText(),\n",
    "                                'street': street_data.getText(),\n",
    "                                'house': houses_data.getText(),\n",
    "                                'provider': prov_name.text,\n",
    "                                'price': price.span.text\n",
    "                            }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to dataframe\n",
    "df = pd.DataFrame(addresses)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-heater",
   "metadata": {},
   "source": [
    "### 5. Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-refrigerator",
   "metadata": {},
   "source": [
    "Hints for data request:\n",
    "1. Proxy\n",
    "2. Exception\n",
    "3. Trials strategy (unlimited or count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url_page, timeout, proxies=None, file=False):\n",
    "    counts = 0\n",
    "    content = None\n",
    "    while counts < MAX_COUNTS:\n",
    "        try:\n",
    "            request = Request(url_page)\n",
    "            request.add_header('User-Agent', USER_AGENT)\n",
    "            if proxies:\n",
    "                proxy_support = ProxyHandler(proxies)\n",
    "                opener = build_opener(proxy_support)\n",
    "                install_opener(opener)\n",
    "                context = ssl._create_unverified_context()\n",
    "                response = urlopen(request, context=context, timeout=timeout)\n",
    "            else:\n",
    "                response = urlopen(request, timeout=timeout)\n",
    "            if file:\n",
    "                content = response.read()\n",
    "            else:\n",
    "                try:\n",
    "                    content = response.read().decode(response.headers.get_content_charset())\n",
    "                except:\n",
    "                    content = None\n",
    "            break\n",
    "        except URLError as e:\n",
    "            counts += 1\n",
    "            print('URLError | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "        except HTTPError as e:\n",
    "            counts += 1\n",
    "            print('HTTPError | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "        except socket.timeout as e:\n",
    "            counts += 1\n",
    "            print('socket timeout | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-store",
   "metadata": {},
   "source": [
    "Example of hints for data search within soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_main = 'https://piter-online.net/address'\n",
    "print(url_main)\n",
    "html = get_content_lite(url_main, timeout=TIMEOUT)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in soup.find_all('div', class_=re.compile(r\"app*\")):\n",
    "    try:\n",
    "        s = x.find('a')['href']\n",
    "        if s.startswith('/address'):\n",
    "            print(s) \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-guard",
   "metadata": {},
   "source": [
    "## LAB WORK #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-click",
   "metadata": {},
   "source": [
    "Rewrite code:\n",
    "1. Add more data about providers' offers (name of the tariff)\n",
    "2. Try to run code NOT in debug mode, collect more data\n",
    "3. Find top provider across every street / region\n",
    "4. Try `heavy` version of `get_content` function (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-winter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
